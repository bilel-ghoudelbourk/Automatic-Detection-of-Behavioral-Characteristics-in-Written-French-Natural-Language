{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_csv('Referentiel_Skilit_Nov-2023.csv', encoding='latin1', sep=';')\n",
    "data_array = data.values\n",
    "data_annotated = data_array[data_array[:, 2] > 0]\n",
    "data_non_annotated = data_array[data_array[:, 2] == 0]\n",
    "print(\"Données annotées:\")\n",
    "print(data_annotated)\n",
    "print(\"\\nDonnées non annotées:\")\n",
    "print(data_non_annotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_axes = np.unique(data_annotated[:, 2])\n",
    "data_by_axis = {}\n",
    "for axis in unique_axes:\n",
    "    data_by_axis[axis] = data_annotated[data_annotated[:, 2] == axis]\n",
    "    \n",
    "print(\"Données pour l'axe 16:\")\n",
    "print(data_by_axis[16])\n",
    "\n",
    "print(\"\\nDonnées pour l'axe 2:\")\n",
    "print(data_by_axis[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Word Embeddings for Words Using CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "import torch\n",
    "\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "def calculate_embeddings(text):\n",
    "    try:\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error When Transforming Text'{text}' into Tensor : {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    if output is not None:\n",
    "        return output.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "transformed_embeddings_by_axis = {}\n",
    "for row in data_annotated:\n",
    "    original_text, transformed_text, original_axis = row\n",
    "    transformed_embedding = calculate_embeddings(transformed_text)\n",
    "    if original_axis not in transformed_embeddings_by_axis:\n",
    "        transformed_embeddings_by_axis[original_axis] = []\n",
    "    transformed_embeddings_by_axis[original_axis].append((transformed_embedding, original_axis))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "#### Here, we transform the 768 dimensions of Camembert Word Embeddings into 2D for visualization using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for axis, emb_list in transformed_embeddings_by_axis.items():\n",
    "    for emb, _ in emb_list:\n",
    "        embeddings.append(emb.cpu().numpy())  \n",
    "        labels.append(axis)\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "cmap = plt.get_cmap('tab20')\n",
    "\n",
    "\n",
    "unique_axes = np.unique(labels)  \n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_axes)))  \n",
    "\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=colors[i], label=axis) for i, axis in enumerate(unique_axes)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(37.5, 30))\n",
    "for i, axis in enumerate(unique_axes):\n",
    "    axis_data = pca_result[labels == axis]\n",
    "    plt.scatter(axis_data[:, 0], axis_data[:, 1], color=colors[i], label=axis)\n",
    "\n",
    "\n",
    "plt.legend(handles=legend_patches, title='Axe')\n",
    "\n",
    "plt.title('Projection of CamemBERT Embeddings Using PCA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we transform the 768 dimensions of Camembert Word Embeddings into 2D for visualization using TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for axis, emb_list in transformed_embeddings_by_axis.items():\n",
    "    for emb, _ in emb_list:\n",
    "        embeddings.append(emb.cpu().numpy())  \n",
    "        labels.append(axis)\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_result = tsne.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "cmap = plt.get_cmap('tab20')\n",
    "\n",
    "\n",
    "unique_axes = np.unique(labels)  \n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_axes)))  \n",
    "\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=colors[i], label=axis) for i, axis in enumerate(unique_axes)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(37.5, 30))\n",
    "for i, axis in enumerate(unique_axes):\n",
    "    axis_data = tsne_result[labels == axis]\n",
    "    plt.scatter(axis_data[:, 0], axis_data[:, 1], color=colors[i], label=axis)\n",
    "\n",
    "\n",
    "plt.legend(handles=legend_patches, title='Axe')\n",
    "\n",
    "plt.title('Projection of CamemBERT Embeddings Using TSNE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroids\n",
    "#### Calculation of the centroids for each of the 16 axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = {}\n",
    "for axis, embeddings_list in transformed_embeddings_by_axis.items():\n",
    "    stack = torch.stack([emb[0] for emb in embeddings_list])  \n",
    "    print(f\"Axe {axis}: {stack.size()}\")\n",
    "    centroids[axis] = torch.mean(stack, dim=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axis, centroid in centroids.items():\n",
    "    print(f\"Centroid of : {axis}:\")\n",
    "    print(centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the centroids in relation to other terms in each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(50, 50))  \n",
    "axs = axs.flatten()  \n",
    "\n",
    "\n",
    "for idx, (axis, embeddings_list) in enumerate(transformed_embeddings_by_axis.items()):\n",
    "    \n",
    "    embeddings_for_axis = torch.stack([emb[0] for emb in embeddings_list])\n",
    "    pca_result_for_axis = pca.fit_transform(embeddings_for_axis.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    centroid_for_axis = centroids[axis].cpu().numpy()\n",
    "    centroid_pca_for_axis = pca.transform([centroid_for_axis])\n",
    "    \n",
    "    \n",
    "    axs[idx].scatter(pca_result_for_axis[:, 0], pca_result_for_axis[:, 1], color='black', alpha=0.5)\n",
    "    \n",
    "    \n",
    "    axs[idx].scatter(centroid_pca_for_axis[0, 0], centroid_pca_for_axis[0, 1], color='red', s=100)\n",
    "    \n",
    "    \n",
    "    axs[idx].set_title(f\"Axe {axis}\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Calculation and Error Rate\n",
    "#### Calculating the distance and error rate to determine whether this method can be used for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_distance_torch(vec1, vec2):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0), dim=1)[0]\n",
    "\n",
    "total_words = 0\n",
    "errors = 0\n",
    "\n",
    "\n",
    "for axis, embeddings_list in transformed_embeddings_by_axis.items():\n",
    "    for transformed_embedding, original_axis in embeddings_list:\n",
    "        total_words += 1\n",
    "        distances = {ax: cosine_distance_torch(transformed_embedding, centroid) for ax, centroid in centroids.items()}\n",
    "        closest_axis = min(distances, key=distances.get)\n",
    "        \n",
    "        if closest_axis != original_axis:\n",
    "            errors += 1\n",
    "\n",
    "\n",
    "error_percentage = (errors / total_words) * 100\n",
    "print(errors)\n",
    "print(total_words)\n",
    "print(f\"Pourcentage des erreurs: {error_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations of Some Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "axes = ['Axe' + str(i) for i in range(1, 5)]  \n",
    "def cosine_distance_torch(vec1, vec2):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0), dim=1)[0]\n",
    "\n",
    "total_words = 0\n",
    "errors = 0\n",
    "example_errors = [] \n",
    "\n",
    "for axis, embeddings_list in transformed_embeddings_by_axis.items():\n",
    "    for transformed_embedding, original_axis in embeddings_list:\n",
    "        total_words += 1\n",
    "        distances = {ax: cosine_distance_torch(transformed_embedding, centroid) for ax, centroid in centroids.items()}\n",
    "        closest_axis = min(distances, key=distances.get)\n",
    "        \n",
    "        if closest_axis != original_axis:\n",
    "            example_errors.append((original_axis, closest_axis, distances))\n",
    "            \n",
    "            \n",
    "            if len(example_errors) == 6:\n",
    "                break\n",
    "    if len(example_errors) == 6:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "selected_errors = example_errors[:6]\n",
    "\n",
    "true_distances = [details[2][details[0]] for details in selected_errors]\n",
    "false_distances = [details[2][details[1]] for details in selected_errors]\n",
    "labels = [f\"True: {details[0]}, False: {details[1]}\" for details in selected_errors]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(true_distances)) \n",
    "width = 0.35 \n",
    "\n",
    "rects1 = ax.bar(x - width/2, true_distances, width, label='True Centroid')\n",
    "rects2 = ax.bar(x + width/2, false_distances, width, label='Closest Incorrect Centroid')\n",
    "\n",
    "ax.set_ylabel('Cosine Distance')\n",
    "ax.set_title('Cosine Distances to True and Closest Incorrect Centroid for Error Examples')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(30, 30)) \n",
    "axes = axes.flatten()\n",
    "\n",
    "error_count = 0\n",
    "axes_used = set()  \n",
    "\n",
    "for axis, embeddings_list in transformed_embeddings_by_axis.items():\n",
    "    for transformed_embedding, original_axis in embeddings_list:\n",
    "        if original_axis in axes_used:\n",
    "            continue  \n",
    "        \n",
    "        distances = {ax: cosine_distance_torch(transformed_embedding, centroid) for ax, centroid in centroids.items()}\n",
    "        closest_axis = min(distances, key=distances.get)\n",
    "\n",
    "        if closest_axis != original_axis:\n",
    "            ax = axes[error_count]\n",
    "            \n",
    "            \n",
    "            original_centroid_2d = centroids[original_axis][:2].numpy()\n",
    "            closest_centroid_2d = centroids[closest_axis][:2].numpy()\n",
    "            transformed_embedding_2d = transformed_embedding[:2].numpy()\n",
    "            \n",
    "            \n",
    "            ax.scatter(*original_centroid_2d, color='green', label=f'Original Centroide: {original_axis}')\n",
    "            ax.scatter(*closest_centroid_2d, color='red', label=f'Closest Centroide: {closest_axis}')\n",
    "            ax.scatter(*transformed_embedding_2d, color='black', label=f'Mot: {axis}')\n",
    "            \n",
    "            \n",
    "            ax.plot([transformed_embedding_2d[0], original_centroid_2d[0]],\n",
    "                    [transformed_embedding_2d[1], original_centroid_2d[1]],\n",
    "                    'g--', label='Distance to Original Centroide')\n",
    "            ax.plot([transformed_embedding_2d[0], closest_centroid_2d[0]],\n",
    "                    [transformed_embedding_2d[1], closest_centroid_2d[1]],\n",
    "                    'r-', label='Distance to Closest Centroide')\n",
    "\n",
    "            error_count += 1\n",
    "            axes_used.add(original_axis)  \n",
    "\n",
    "            if error_count == 16:\n",
    "                break  \n",
    "\n",
    "    if error_count == 16:\n",
    "        break\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < error_count:\n",
    "        ax.set_title(f'Error in Axis {list(transformed_embeddings_by_axis.keys())[i]}')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm  \n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(30, 30))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "error_count = 0\n",
    "axes_used = set()  \n",
    "\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(centroids)))\n",
    "\n",
    "for axis, embeddings_list in transformed_embeddings_by_axis.items():\n",
    "    if error_count >= 16:\n",
    "        break  \n",
    "\n",
    "    for transformed_embedding, original_axis in embeddings_list:\n",
    "        if original_axis in axes_used:\n",
    "            continue  \n",
    "\n",
    "        distances = {ax: cosine_distance_torch(transformed_embedding, centroid) for ax, centroid in centroids.items()}\n",
    "        closest_axis = min(distances, key=distances.get)\n",
    "\n",
    "        if closest_axis != original_axis:\n",
    "            ax = axes[error_count]\n",
    "            \n",
    "            \n",
    "            centroid_positions = {ax: centroids[ax][:2].numpy() for ax in centroids}\n",
    "\n",
    "            \n",
    "            for idx, (centroid_label, pos) in enumerate(centroid_positions.items()):\n",
    "                ax.scatter(*pos, color=colors[idx], s=100)\n",
    "\n",
    "            \n",
    "            transformed_embedding_2d = transformed_embedding[:2].numpy()\n",
    "            ax.scatter(*transformed_embedding_2d, color='black', s=150)\n",
    "\n",
    "            \n",
    "            for idx, (centroid_label, pos) in enumerate(centroid_positions.items()):\n",
    "                line_style = '--' if centroid_label == original_axis else '-'\n",
    "                color_='black' if centroid_label == original_axis else colors[idx]\n",
    "                ax.plot([transformed_embedding_2d[0], pos[0]],\n",
    "                        [transformed_embedding_2d[1], pos[1]],\n",
    "                        color=color_, linestyle=line_style)\n",
    "\n",
    "            axes_used.add(original_axis)\n",
    "            error_count += 1\n",
    "\n",
    "            if error_count >= 16:\n",
    "                break  \n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < error_count:\n",
    "        ax.set_title(f'Error in Axis {list(transformed_embeddings_by_axis.keys())[i]}')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
